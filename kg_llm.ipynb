{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import json\n",
        "\n",
        "# Load entity mappings from entity2text.txt\n",
        "entity_dict = {}\n",
        "with open('entity2text.txt', 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        entity_id, entity_text = line.strip().split('\\t')\n",
        "        entity_dict[entity_id] = entity_text\n",
        "\n",
        "# Load relation mappings from alignment_clean.txt\n",
        "\n",
        "with open('alignment_clean.txt', 'r', encoding='utf-8') as f:\n",
        "    relation_dict = json.load(f)\n",
        "\n",
        "def replace_in_file(input_file, output_file, max_lines=None):\n",
        "    with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
        "        for i, line in enumerate(infile):\n",
        "            if max_lines is not None and i >= max_lines:\n",
        "                break\n",
        "\n",
        "            parts = line.strip().split('\\t')\n",
        "\n",
        "            # Map the head, relation, and tail entities\n",
        "            head_entity = entity_dict.get(parts[0], parts[0])\n",
        "            relation_description = relation_dict.get(parts[1], parts[1])\n",
        "            tail_entity = entity_dict.get(parts[2], parts[2])\n",
        "\n",
        "            # Write each entry to the output file\n",
        "            outfile.write(f\"Head: {head_entity}\\nRelation: {relation_description}\\nTail: {tail_entity}\\n\\n\")\n",
        "\n",
        "# Replace entities and relations in train.tsv\n",
        "replace_in_file('train.tsv', 'train_replaced.txt')\n",
        "\n",
        "# Replace entities and relations in test.tsv\n",
        "replace_in_file('test.tsv', 'test_replaced.txt')"
      ],
      "metadata": {
        "id": "Z-HFNMk7H1uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_entities_from_train(train_file, max_lines=100000):\n",
        "    entities = set()\n",
        "    with open(train_file, 'r', encoding='utf-8') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i >= max_lines:\n",
        "                break\n",
        "            parts = line.strip().split('\\t')\n",
        "            if len(parts) >= 3:\n",
        "                head_entity, tail_entity = parts[0], parts[2]\n",
        "                entities.add(head_entity)\n",
        "                entities.add(tail_entity)\n",
        "    return entities\n",
        "\n",
        "def filter_test_file(test_file, output_file, train_entities):\n",
        "    with open(test_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
        "        for line in infile:\n",
        "            parts = line.strip().split('\\t')\n",
        "            if len(parts) >= 3:\n",
        "                head_entity, tail_entity = parts[0], parts[2]\n",
        "                if head_entity in train_entities and tail_entity in train_entities:\n",
        "                    outfile.write(line)\n",
        "\n",
        "# Load entities from the first 100,000 rows of train.tsv\n",
        "train_entities = load_entities_from_train('train.tsv')\n",
        "\n",
        "#train_entities = random.sample(train_entities, 200)\n",
        "\n",
        "# Filter test.tsv based on these entities\n",
        "filter_test_file('test.tsv', 'filtered_test.tsv', train_entities)"
      ],
      "metadata": {
        "id": "G4ErOe0UN96i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def load_entities_from_train(train_file, max_lines=100000):\n",
        "    entities = set()\n",
        "    with open(train_file, 'r', encoding='utf-8') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i >= max_lines:\n",
        "                break\n",
        "            parts = line.strip().split('\\t')\n",
        "            if len(parts) >= 3:\n",
        "                head_entity, tail_entity = parts[0], parts[2]\n",
        "                entities.add(head_entity)\n",
        "                entities.add(tail_entity)\n",
        "    return entities\n",
        "\n",
        "def filter_test_file(test_file, output_file, train_entities):\n",
        "    unique_relations = set()\n",
        "    with open(test_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
        "        for line in infile:\n",
        "            parts = line.strip().split('\\t')\n",
        "            if len(parts) >= 3:\n",
        "                head_entity, relation, tail_entity = parts[0], parts[1], parts[2]\n",
        "                if head_entity in train_entities and tail_entity in train_entities:\n",
        "                    unique_relations.add(relation)\n",
        "                    outfile.write(line)\n",
        "    return len(unique_relations)\n",
        "\n",
        "# Load entities from the first 100,000 rows of train.tsv\n",
        "train_entities = load_entities_from_train('train.tsv')\n",
        "\n",
        "# Iteratively sample entities until we achieve around 100 unique relations\n",
        "desired_relations_count = 100\n",
        "sample_size = 10\n",
        "\n",
        "while True:\n",
        "    # Randomly sample a subset of entities\n",
        "    sampled_entities = random.sample(train_entities, min(sample_size, len(train_entities)))\n",
        "\n",
        "    # Filter test.tsv based on these sampled entities\n",
        "    num_relations = filter_test_file('test.tsv', 'filtered_test.tsv', sampled_entities)\n",
        "\n",
        "    # Check if we have around 100 unique relations\n",
        "    if num_relations >= desired_relations_count:\n",
        "        print(f\"Achieved {num_relations} unique relations with {len(sampled_entities)} sampled entities.\")\n",
        "        break\n",
        "\n",
        "    # Increase sample size incrementally to reach desired relation count\n",
        "    sample_size += 10\n",
        "\n",
        "# Output the final set of sampled entities\n",
        "print(\"Final set of sampled entities:\", sampled_entities)"
      ],
      "metadata": {
        "id": "seBeoq1jS0x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load entity mappings from entity2text.txt\n",
        "entity_dict = {}\n",
        "with open('entity2text.txt', 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        entity_id, entity_text = line.strip().split('\\t')\n",
        "        entity_dict[entity_id] = entity_text\n",
        "\n",
        "# Load relation mappings from alignment_clean.txt\n",
        "with open('alignment_clean.txt', 'r', encoding='utf-8') as f:\n",
        "    relation_dict = json.load(f)\n",
        "\n",
        "def replace_and_format_for_llm(input_file, output_file):\n",
        "    with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
        "        for line in infile:\n",
        "            parts = line.strip().split('\\t')\n",
        "            if len(parts) >= 3:\n",
        "                head_entity_id, relation_id = parts[0], parts[1]\n",
        "\n",
        "                # Replace IDs with text descriptions\n",
        "                head_entity = entity_dict.get(head_entity_id, head_entity_id)\n",
        "                relation_description = relation_dict.get(relation_id, relation_id)\n",
        "\n",
        "                # Write the formatted entry to the output file\n",
        "                outfile.write(f\"Head: {head_entity}\\nRelation: {relation_description}\\nTail: [Predict the missing entity]\\n\\n\")\n",
        "\n",
        "# Replace entities and relations in test.tsv and prepare for LLM\n",
        "replace_and_format_for_llm('filtered_test.tsv', 'llm_input.txt')"
      ],
      "metadata": {
        "id": "zXNNgCxmULLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_tails_from_predict(predict_file):\n",
        "    tails = []\n",
        "    with open(predict_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if line.startswith('Tail: '):\n",
        "                tail = line[len('Tail: '):].strip()\n",
        "                tails.append(tail)\n",
        "    return tails\n",
        "\n",
        "def extract_tails_from_test(test_file, num_rows=55):\n",
        "    tails = []\n",
        "    with open(test_file, 'r', encoding='utf-8') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i >= num_rows:\n",
        "                break\n",
        "            parts = line.strip().split('\\t')\n",
        "            if len(parts) >= 3:\n",
        "                tail_entity = parts[2]\n",
        "                tails.append(tail_entity)\n",
        "    return tails\n",
        "\n",
        "def match_tails(predict_tails, test_tails):\n",
        "    matches = []\n",
        "    for test_tail in test_tails:\n",
        "        if test_tail in predict_tails:\n",
        "            matches.append(test_tail)\n",
        "    return matches\n",
        "\n",
        "# Extract tails from predict.txt\n",
        "predict_tails = extract_tails_from_predict('predict.txt')\n",
        "\n",
        "# Extract tails from the first 55 rows of test.tsv\n",
        "test_tails = extract_tails_from_test('/content/test_replaced1.tsv')\n",
        "\n",
        "# Match tails between predict.txt and test.tsv\n",
        "matched_tails = match_tails(predict_tails, test_tails)\n",
        "\n",
        "# Output the matched tails\n",
        "print(\"Matched Tails:\", matched_tails)"
      ],
      "metadata": {
        "id": "gM1mhr-lWZhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_tails_from_predict(predict_file):\n",
        "    tails = []\n",
        "    with open(predict_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if line.startswith('Tail: '):\n",
        "                tail = line[len('Tail: '):].strip()\n",
        "                tails.append(tail)\n",
        "    return tails\n",
        "\n",
        "def extract_tails_from_test(test_file, num_rows=55):\n",
        "    tails = []\n",
        "    with open(test_file, 'r', encoding='utf-8') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i >= num_rows:\n",
        "                break\n",
        "            parts = line.strip().split('\\t')\n",
        "            if len(parts) >= 3:\n",
        "                tail_entity = parts[2]\n",
        "                tails.append(tail_entity)\n",
        "    return tails\n",
        "\n",
        "def match_tails(predict_tails, test_tails):\n",
        "    matches = [test_tail for test_tail in test_tails if test_tail in predict_tails]\n",
        "    return matches\n",
        "\n",
        "def calculate_accuracy(matched_tails, total_tails):\n",
        "    if total_tails == 0:\n",
        "        return 0.0\n",
        "    return (len(matched_tails) / total_tails) * 100\n",
        "\n",
        "# Extract tails from predict.txt\n",
        "predict_tails = extract_tails_from_predict('predict.txt')\n",
        "\n",
        "# Extract tails from the first 55 rows of test.tsv\n",
        "test_tails = extract_tails_from_test('test_replaced1.tsv')\n",
        "\n",
        "# Match tails between predict.txt and test.tsv\n",
        "matched_tails = match_tails(predict_tails, test_tails)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = calculate_accuracy(matched_tails, len(test_tails))\n",
        "\n",
        "# Output the matched tails and accuracy\n",
        "print(\"Matched Tails:\", matched_tails)\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "nwDNsWdQqp2J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}